{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Kainat yek vücut, tek varlıktır. Her şey ve herkes görünmez iplerle birbirine bağlıdır. Sakın kimsenin ahını alma, hele hele kendinden zayıf olanın canını yakma. Unutma ki, dünyanın öte ucunda tek bir insanın kederi, tüm insanlığı mutsuz edebilir. Ve bir kişinin saadeti, herkesin yüzünü güldürebilir.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kainat',\n",
       " 'yek',\n",
       " 'vücut,',\n",
       " 'tek',\n",
       " 'varlıktır.',\n",
       " 'Her',\n",
       " 'şey',\n",
       " 've',\n",
       " 'herkes',\n",
       " 'görünmez',\n",
       " 'iplerle',\n",
       " 'birbirine',\n",
       " 'bağlıdır.',\n",
       " 'Sakın',\n",
       " 'kimsenin',\n",
       " 'ahını',\n",
       " 'alma,',\n",
       " 'hele',\n",
       " 'hele',\n",
       " 'kendinden',\n",
       " 'zayıf',\n",
       " 'olanın',\n",
       " 'canını',\n",
       " 'yakma.',\n",
       " 'Unutma',\n",
       " 'ki,',\n",
       " 'dünyanın',\n",
       " 'öte',\n",
       " 'ucunda',\n",
       " 'tek',\n",
       " 'bir',\n",
       " 'insanın',\n",
       " 'kederi,',\n",
       " 'tüm',\n",
       " 'insanlığı',\n",
       " 'mutsuz',\n",
       " 'edebilir.',\n",
       " 'Ve',\n",
       " 'bir',\n",
       " 'kişinin',\n",
       " 'saadeti,',\n",
       " 'herkesin',\n",
       " 'yüzünü',\n",
       " 'güldürebilir.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zeysert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Kainat',\n",
       " 'yek',\n",
       " 'vücut',\n",
       " ',',\n",
       " 'tek',\n",
       " 'varlıktır',\n",
       " '.',\n",
       " 'Her',\n",
       " 'şey',\n",
       " 've',\n",
       " 'herkes',\n",
       " 'görünmez',\n",
       " 'iplerle',\n",
       " 'birbirine',\n",
       " 'bağlıdır',\n",
       " '.',\n",
       " 'Sakın',\n",
       " 'kimsenin',\n",
       " 'ahını',\n",
       " 'alma',\n",
       " ',',\n",
       " 'hele',\n",
       " 'hele',\n",
       " 'kendinden',\n",
       " 'zayıf',\n",
       " 'olanın',\n",
       " 'canını',\n",
       " 'yakma',\n",
       " '.',\n",
       " 'Unutma',\n",
       " 'ki',\n",
       " ',',\n",
       " 'dünyanın',\n",
       " 'öte',\n",
       " 'ucunda',\n",
       " 'tek',\n",
       " 'bir',\n",
       " 'insanın',\n",
       " 'kederi',\n",
       " ',',\n",
       " 'tüm',\n",
       " 'insanlığı',\n",
       " 'mutsuz',\n",
       " 'edebilir',\n",
       " '.',\n",
       " 'Ve',\n",
       " 'bir',\n",
       " 'kişinin',\n",
       " 'saadeti',\n",
       " ',',\n",
       " 'herkesin',\n",
       " 'yüzünü',\n",
       " 'güldürebilir',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kainat yek vücut, tek varlıktır.',\n",
       " 'Her şey ve herkes görünmez iplerle birbirine bağlıdır.',\n",
       " 'Sakın kimsenin ahını alma, hele hele kendinden zayıf olanın canını yakma.',\n",
       " 'Unutma ki, dünyanın öte ucunda tek bir insanın kederi, tüm insanlığı mutsuz edebilir.',\n",
       " 'Ve bir kişinin saadeti, herkesin yüzünü güldürebilir.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kainat\n",
      "yek\n",
      "vücut\n",
      ",\n",
      "tek\n",
      "varlıktır\n",
      ".\n",
      "Her\n",
      "şey\n",
      "ve\n",
      "herkes\n",
      "görünmez\n",
      "iplerle\n",
      "birbirine\n",
      "bağlıdır\n",
      ".\n",
      "Sakın\n",
      "kimsenin\n",
      "ahını\n",
      "alma\n",
      ",\n",
      "hele\n",
      "hele\n",
      "kendinden\n",
      "zayıf\n",
      "olanın\n",
      "canını\n",
      "yakma\n",
      ".\n",
      "Unutma\n",
      "ki\n",
      ",\n",
      "dünyanın\n",
      "öte\n",
      "ucunda\n",
      "tek\n",
      "bir\n",
      "insanın\n",
      "kederi\n",
      ",\n",
      "tüm\n",
      "insanlığı\n",
      "mutsuz\n",
      "edebilir\n",
      ".\n",
      "Ve\n",
      "bir\n",
      "kişinin\n",
      "saadeti\n",
      ",\n",
      "herkesin\n",
      "yüzünü\n",
      "güldürebilir\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in word_tokenize(text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'While everyone is trying to be something in this world, you be nothing. Let your range be absent. It is not self-consciousness, but nothingness consciousness that keeps people standing.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zeysert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('stopwords')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_words = []\n",
    "for word in words:\n",
    "    if word not in stopwords:\n",
    "        filtered_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['While',\n",
       " 'everyone',\n",
       " 'trying',\n",
       " 'something',\n",
       " 'world',\n",
       " ',',\n",
       " 'nothing',\n",
       " '.',\n",
       " 'Let',\n",
       " 'range',\n",
       " 'absent',\n",
       " '.',\n",
       " 'It',\n",
       " 'self-consciousness',\n",
       " ',',\n",
       " 'nothingness',\n",
       " 'consciousness',\n",
       " 'keeps',\n",
       " 'people',\n",
       " 'standing',\n",
       " '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive\n",
      "drive\n",
      "driver\n",
      "drive\n",
      "drove\n",
      "cat\n",
      "children\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'For many people, trust means to remain passive. However, trust is; it is a state of peace brought by acceptance and harmony. It is not passive but active.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\zeysert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For', 'IN'),\n",
       " ('many', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " (',', ','),\n",
       " ('trust', 'NN'),\n",
       " ('means', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('remain', 'VB'),\n",
       " ('passive', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('However', 'RB'),\n",
       " (',', ','),\n",
       " ('trust', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " (';', ':'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('state', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('peace', 'NN'),\n",
       " ('brought', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('acceptance', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('harmony', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('passive', 'JJ'),\n",
       " ('but', 'CC'),\n",
       " ('active', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Surrender instead of resisting the changes that life faces. Let your life be with you, not with you. Do not worry that your order will be disrupted, it will be over your life. How do you know that your life will not be better than gold?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\zeysert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\zeysert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_ent = nltk.ne_chunk(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_ent.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    print(lem.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem.lemmatize('drove', 'v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
